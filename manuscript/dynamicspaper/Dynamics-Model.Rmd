---
title: Using Switching State-Space Models to Interpret Musical Dynamics

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Robert Granger
  thanks: The authors gratefully acknowledge ...
  affiliation: Department of Statistics, Indiana University
  

keywords:
- 3 to 6 keywords
- that do not appear in the title

abstract: |
  The text of your abstract.  200 or fewer words.
  
bibliography: musicdynamicsbib.bib
header-includes:
   - \usepackage{amsmath}
   - \usepackage{booktabs}
   - \usepackage{pgf}
   - \usepackage{tikz}
   - \usepackage{algorithm2e}

output: rticles::asa_article
---

# Introduction

\def\algorithmautorefname{Algorithm}

Things to write about in this section

1) Classifying music performances to sort on personal preference

2) Smoothing performances to sound better from Midi data

3) Using State-Switching Model

The composer gives directions about dynamics. $p$ stands for "piano" and indicates this section should be played "soft".  The "f" stands for "forte" and indicates this section should be played 


\section{The Switching State-Space Model}
\label{sec:model}

State-Space models are commonly used to model time series observations in the presence of perceived hidden, continuous states.  As a result of the state-space framework, the observations are viewed as independent conditional on the hidden states whereas these hidden states will follow a vector autoregressive process. Adding assumptions of linearity and Gaussian error produces the following model commonly referred to as the general linear Gaussian state-space model @durbin_time_2012 takes the form
\begin{equation}
  \begin{aligned}
    y_t &= C_t + D_tx_t + \epsilon_t, 
    & \epsilon_t & \sim N(0,G_t)\\
    x_{t+1} &= A_t + B_tx_t + \eta_t, 
    & \eta_t & \sim N(0,H_t), 
    & x_1 & \sim N(x_0,P_0) \\
  \end{aligned}
  \label{eq:statespacemod}
\end{equation}
where the first part of \autoref{eq:statespacemod} is known as the observation equation and the second part is known as the state equation. $y_t$ is a vector of known observations, and $x_t$ is a vector of the unobserved, continuous states at each time period, $t$.  The observation error, $\epsilon_t$, and the state equation error, $\eta_t$, are both assumed to be independent and identically distributed.  

In the typical state-space framework, the matrices $A_t$, $B_t$, $C_t$, $D_t$, $G_t$, and $H_t$ are allowed to vary across time but are known. If there are a finite number of perceived structures for these matrices, and the given structure is unknown at time, $t$, a switching state-space model can be used.  This model assumes there are some underlying discrete states, $s_i$, that transition over time through a Markov process.  Making this slight adjustment to \autoref{eq:statespacemod} yields the following model which will be used as a basic framework for modeling music dynamics.

\begin{equation}
  \begin{aligned}
    y_t &= C_t(s_t) + D_t(s_t)x_t + \epsilon_t, 
    & \epsilon_t & \sim N(0,G_t)\\
    x_{t+1} &= A_t(s_t) + B_t(s_t)x_t + \eta_t, 
    & \eta_t & \sim N(0,H_t), 
    & x_1 & \sim N(x_0,P_0) \\
  \end{aligned}
  \label{eq:switchstatemodel}
\end{equation}

When it comes to musical dynamics, rarely do musicians attempt to play with the same dynamics or loudness throughout the entirety of a piece.  While there may be dramatic changes occasionally throughout the performance, most of the time we expect the musician to steadily change the loudness from note to note. Furthermore, the musician may quietly or loudly play individual notes for emphasis.  In order to model this behavior, we propose the following four discrete states:

\begin{list}{}{}

\item[$s^1$:] The musician selects a new value for loudness.

\item[$s^2$:] The musician continues the dynamics in a steady way.

\item[$s^3$:] The musician plays a single note more loudly.

\item[$s^4$:] The musician plays a single note more softly.

\end{list}

The observation, $y_t$ is the univariate loudness of the note at each time period, $t$.  In order to allow the dynamics to progress steadily, the continuous hidden states, $x_t$, follow a process that allows for piece-wise quadratic displays.  At each time period, $x_t$, is an array of length three,
$$x_t = (x^0_t, x^1_t, x^2_t), $$
where $x^0_t$ is the loudness, $x^1_t$ is the first order difference, and $x^2_t$ is the second order difference.  Since we want to maintain this smooth progression even when the musician plays a single note more loudly or softly, states $s^3$ and $s^4$ are implemented by adding a constant in the observation equation as opposed to changing the state equation.  The model is similar to that proposed in @gu_modeling_2012, but extended to include these additional states.  \autoref{tab:parmats} shows the parameter matrices for the four states.  

The final part of the switching state-space model is designing the Markov process for transitioning between the discrete states. \autoref{fig:transmat} displays the structure of the transition between states.  The first state, $s^1$, allows for the selection of a new loudness which then transitions into the smooth progresson state, $s^2$, with probability 1.  When arriving in $s^2$, the next time period's discrete state can be any of the possible states including itself.  If in this progression, we move to a sudden loud note, $s^3$, or a sudden soft note, $s^4$, then we have the opportunity to continue the smooth progression, $s^2$, or start a new smooth progression, $s^1$. It is not permissible though to immediately then play another sudden loud or soft note. With four states, the transition matrix could potentially have 12 probabilities to estimate; however, because of restrictions placed on the possible transitions, only 5 probabilities need to be estimated.

In total, there are 13 distinct unknown parameters, $\theta \in \Theta$, spread across 4 unknown states, $s^i \in S$, through $T$ time periods. A summary of the parameters to be estimated are: $\Theta = \{\mu_c, \sigma^2_\epsilon, \mu_0, \mu_1, \mu_2, \sigma^2_0, \sigma^2_1, \sigma^2_2, p_{21}, p_{23}, p_{24}, p_{31}, p_{41}\}$. In the next section, we discuss finding smoothed estimates of the dynamics which requires estimation of these parameters along with the continous and discrete states.

\begin{table}
\centering
\begin{tabular}[h!]{@{}llccccccc@{}}
\toprule
%&&&\multicolumn{3}{c}{Parameter Matrices}\\
  \multicolumn{2}{c}{States} &\phantom{a}& \multicolumn{6}{c}{Parameter Matrices}\\
  \cmidrule{1-2} \cmidrule{4-9}
  $S$ &&& $A$ & $B$ & $C$ & $D$ & $G$ & $H$ \\
  \midrule
  $s^1$ & && $\begin{pmatrix} \mu_0 \\ \mu_1 \\ \mu_2 \end{pmatrix}$ & $\begin{pmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&0 \end{pmatrix}$ & 0 & $\begin{pmatrix} 1\\ 0 \\ 0 \end{pmatrix}$ & $\sigma_\epsilon^2$ & $\begin{pmatrix} \sigma_0^2&0&0 \\ 0&\sigma_1^2&0 \\ 0&0&\sigma_2^2 \end{pmatrix}$\\
  \\
  $s^2$ & && $\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ & $\begin{pmatrix} 1&1&0 \\ 0&1&1 \\ 0&0&1 \end{pmatrix}$ & 0 & $\begin{pmatrix} 1\\ 0 \\ 0 \end{pmatrix}$ & $\sigma_\epsilon^2$ & $\begin{pmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&0 \end{pmatrix}$\\
  \\
  $s^3$ & && $\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ & $\begin{pmatrix} 1&1&0 \\ 0&1&1 \\ 0&0&1 \end{pmatrix}$ & $\mu_c$ & $\begin{pmatrix} 1\\ 0 \\ 0 \end{pmatrix}$ & $\sigma_\epsilon^2$ & $\begin{pmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&0 \end{pmatrix}$\\
  \\
  $s^4$ & && $\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ & $\begin{pmatrix} 1&1&0 \\ 0&1&1 \\ 0&0&1 \end{pmatrix}$ & $-\mu_c$ & $\begin{pmatrix} 1\\ 0 \\ 0 \end{pmatrix}$ & $\sigma_\epsilon^2$ & $\begin{pmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&0 \end{pmatrix}$\\

\bottomrule
\end{tabular}
\caption{Parameter matrices for the switching state space model.\label{tab:parmats}}
\end{table}

\begin{figure}[tb!]
  \centering
  \tikzstyle{switch}=[rectangle,
  thick, minimum size=1cm, draw=black]
  \begin{tikzpicture}[>=latex,text height=1.5ex,text depth=0.25ex]
    \matrix[row sep=0.25cm,column sep=.5cm] {
      &&& \node (S1) [switch] {$s_1$};&&& \\
      \\ \\ \\ \\ \\ \\
      &&& \node (S2) [switch] {$s_2$};&&& \\
      \\ \\ \\ \\ \\ \\
      &\node (S3) [switch] {$s_3$}; &&&& \node (S4) [switch] {$s_4$};\\
    };
    \path[->]
    (S1) edge [bend right] node [left] {1}(S2)
    (S2) edge [bend right] node [right] {$p_{21}$}(S1)
    (S2) edge [loop above] node [left] {}(S2)
    (S2) edge [bend right] node [right] {$p_{23}$}(S3)
    (S2) edge [bend right] node [right] {$p_{24}$}(S4)
    (S3) edge [bend left] node [left] {$p_{31}$}(S1)
    (S3) edge [bend right] node [left] {}(S2)
    (S4) edge [bend right] node [right] {$p_{41}$}(S1)
    (S4) edge [bend right] node [left] {}(S2);
  \end{tikzpicture}
  \caption{Transition diagram. \label{fig:transmat}}
\end{figure}



\section{Evaluating the Model}
\label{sec:analysis}

\subsection{The algorithm}

The construction of the model is designed to allow us to separate the observed dynamics, $y_t$, from the performer's intended dynamics, $E[y_t]$. Hence, the aim in estimating the intended dynamics is attempting to remove $\epsilon_t$, which can be seen as unintended deviations from the desired loudness. 

When the parameter values, $\theta_i \in \Theta$, and the discrete states, $S$, are known, the Kalman filter (see @kalman_new_1960) can be used to find estimates of the continuous hidden states, $\{x_t\}_{t=0}^t$, along with the likelihood of $\Theta$.  While the Kalman filter provides an easy way to compute the likelihood, the estimate of $x_t$ is obtained using only observations coming before time $t$.  In order to use all information, we implement the Kalman smoothing algorithm introduced in @rauch_maximum_1965.  Given $\Theta$ and $S$, this algorithm provides an estimate, $\hat{x_t} = E[x_t|y_0,...y_T]$, which can be used to compute a fitted or "smoothed" value, $\hat{y_t}$.

So far, if $\Theta$ and $S$ are known, we have a closed form solution to obtaining the smoothed dynamics through the Kalman smoother.  If $\Theta$ is known, we can obtain the most likely set of discrete states, $S$, by simply running the Kalman smoother algorithm on every possible combination of discrete states and discovering the largest likelihood.  This may work if the number of discrete states and/or time periods is small; however, the number of state combinations to check would be $\|\{s_i \in S\}\|^T$.  For example, if we were using the music dynamics model presented in the previous section on Chopin's Mazurka Op. 63 No. 3 (which has 231 notes), the total number of state combinations to check would be $4^{231}\approx1.19\times 10^{139}$.  Of course, many of these state combinations could be removed due to 0 likelihood given the restrictions on the state transitions, but even if this is taken into account, the number of state combinations would be far too large.  To overcome this issue, we only look at a subset of these combinations through the Discrete Particle Filter as described in @mcdonald_markov-switching_2019.  The algorithm works by estimating the partial likelihood iteratively through time at each of the possible discrete state paths using the Kalman Filter.  Each of these paths is known as a particle with only the previous states and partial likelihood being saved.  This process would still require checking all $\{\#s_i \in S\}^T$ paths, so to get around this, a maximum number of particles to save at each time iteration is selected. 

\begin{figure}[tb!]
  \centering
  \tikzstyle{switch}=[rectangle,
  thick, minimum size=0.5cm, draw=black]
  \begin{tikzpicture}[>=latex,text height=1.5ex,text depth=0.25ex]
    \matrix[row sep=0.25cm,column sep=.75cm] {
      \node (S1) [switch] {$s_1$}; &&\node (S4) [switch] {$s_1s_1$};&& \node (S7) [switch] {$s_1s_1$}; && \node (S8) [switch] {$s_1s_1s_1$}; && \node (S9) [switch] {$s_1s_1s_1$}; &&\\
      \\
      && && && \node (S13) [switch] {$s_1s_1s_2$};\\
      \\
       &&\node (S3) [switch] {$s_1s_2$}; && \node (S10) [switch] {$s_1s_2$}; && \node (S14) [switch] {$s_1s_2s_1$}; && \node (S20) [switch] {$s_1s_2s_1$}; \\
      \\ 
             && &&  && \node (S15) [switch] {$s_1s_2s_2$}; && \node (S21) [switch] {$s_2s_2s_2$}; \\
      \\
      \node (S2) [switch] {$s_2$}; &&\node (S5) [switch] {$s_2s_1$};&&\node (S11) [switch] {$s_2s_1$};&& \node (S16) [switch] {$s_2s_1s_1$}; && \node (S22) [switch] {$s_2s_1s_1$}; \\
      \\
      && && && \node (S17) [switch] {$s_2s_1s_2$};&&\node (S23) [switch] {$s_2s_1s_2$};
      \\ 
       &&\node (S6) [switch] {$s_2s_2$};&&\node (S12) [switch] {$s_2s_2$};&& \node (S18) [switch] {$s_2s_2s_1$};\\
      \\
      &&&&&& \node (S19) [switch] {$s_2s_2s_2$}; &&\\
      \\
      \hline
      \\
      \\
      $t=1$ && && \hspace{12px} $t=2$ && && \hspace{52px} $t=3$  \\
    };
    \path[->]
    (S1) edge (S3)
    (S7) edge (S8)
    (S7) edge (S13)
    (S8) edge [dashed] (S9)
    (S10) edge (S14)
    (S10) edge (S15)
    (S11) edge (S16)
    (S11) edge (S17)
    (S4) edge [dashed] (S7)
    (S1) edge (S4)
    (S2) edge (S5)
    (S12) edge (S18)
    (S12) edge (S19)
    (S3) edge [dashed] (S10)
    (S5) edge [dashed] (S11)
    (S6) edge [dashed] (S12)
    (S14) edge [dashed] (S20)
    (S15) edge [dashed] (S21)
    (S16) edge [dashed] (S22)
    (S17) edge [dashed] (S23)
    (S2) edge (S6);
  \end{tikzpicture}
  \caption{Discrete Particle Filter with 2 discrete states and capping the maximum number of stored particles at 5 for each time iteration. \label{fig:dpf}}
\end{figure}

\autoref{fig:dpf} illustrates this concept with 2 discrete states and the maximum number of particles set at 5.  There are four possible state paths to check when passing from $t=1$ to $t=2$ which is less than the maximum particle number so all four paths are saved.  When moving from $t=2$ to $t=3$, the number of paths increases to 8.  Since only 5 paths are allowed, three of these paths must be dropped and will no longer be considered as possible solutions.  In order to decide which paths are to be saved a sampling procedure must be performed.  One sampling procedure proposed by @tugnait_detection_1982 is to simply keep the paths that have the highest likelihood.  Another sampling procedure proposed by @akashi_random_1977 is to randomly sample one particle out of the total number of states descended from each of the $t-1$ particles in proportion to their respective likelihoods. @fearnhead_-line_2003 proposes another stochastic approach but one that minimizes the expected mean squared error.  This approach determines a threshold value such that all particles with likelihood above this value are kept and the remaining particles to be kept are chosen at random with probability equal to their respective likelihoods.

The final step is estimating the model parameters, $\Theta$, that maximize the likelihood. This is no easy task as this model presents a couple of problems: 1) many of the parameters are constrained in our model as variances need to be positive and the probabilities should be between 0 and 1; 2) there are many local maxima making finding a global maxima difficult. To help alleviate these concerns, we can implement Bayesian priors on our parameters.  Using carefully selected priors allows us to steer the algorithm away from impossible solutions and direct it toward more desired solutions.  These issues also lead us to carefully consider our optimization technique.  We will use two different methods.  First, we will implement Nelder-Mead which can find local maxima for unconstrained multidimensional problems without the need for derivative computation @nelder_simplex_1965.  Second, we will implement Simulated Annealing (SANN) which is commonly used when dealing with problems with many local maxima in order to estimate the global maxima. For both of these methods, the selection of initial values can greatly determine the outcome, so many should be selected and tried.  The best solution found, regardless of the algorithm or initial values, shall be deemed the approximate solution.

The complete algorithm can be found in \autoref{alg:masteralgorithm}.  The solution to this model was solved using software R @r_core_team_r_2019.  The Nelder-Mead optimization is performed using the package \texttt{optimr} by @nash_optimr_2019 and the discrete particle filter was implemented by extending the package \texttt{dpf} by @mcdonald_dpf_2020.

\begin{algorithm}[H]
 \caption{Algorithm for Solving Music Dynamics Model\label{alg:masteralgorithm}}
\SetAlgoLined
 \textbf{Input} Y, a vector of observed dynamics\;
 \textbf{Initialize} $\Theta$\;
 \While{(Stopping Criteria)}{
 Create Matrices A, B, C, D, G, H at each $t$ for each $s^i \in S$\;
 Implement Discrete Particle Filter\;
 Compute the log-likelihood: \hspace{2px} $l(Y|\Theta,S) + l(S|\Theta) + l(\Theta)$\;
 Update $\Theta$ via Nelder-Mead Optimization or SANN\;
 }
\end{algorithm}

\subsection{Results for Chopin's Mazurka Op. 63 No. 3}

The model was fit using observed dynamics from 46 different performances of Chopin's Mazurka Op. 63. No 3.  This data was created by Andrew Earis @earis_mazurka_2009 and found on the website for the Centre for the History and Analysis of Recorded Music (CHARM) @charm_centre_2009.  The technique used to create the data is described in detail in his paper "An algorithm to extra expressive timing and dynamics from piano recordings" @earis_algorithm_2007.

To implement \autoref{alg:masteralgorithm} on dynamics data from performances of Chopin's Mazurka Op. 63 No.3, we must make a few specific decisons.  First, we choose prior distributions applicable specifically to Chopin's Mazurka Op. 63 No.3, which can be found in \autoref{tab:priors}.  Secondly, regarding the Discrete Partile Filter, we selected to keep up to 500 particles with the sampling criterion of keeping the largest likeliehoods.  Lastily, since there are many local minima, we used 20 different starting points drawn randomly and independently from each of the prior distributions per optimization technique (both Nelder-Mead and SANN).

\begin{table}[t]
  \centering
  \begin{tabular}{@{}rcll@{}}
    \toprule
    Parameter & \phantom{a} & Distribution & Prior Mean \\
    \midrule
    $\mu_c$ & $\sim$ & Gamma$(100,\ 0.1)$ & 10\\
    $\sigma^2_{\epsilon}$ & $\sim$ & Gamma$(10,\ 0.5)$ & 5\\
    $\mu_{0}$ & $\sim$ & Normal$(\overline{Y},\ 10)$ & $\overline{Y}$\\
    $\mu_{1} $ & $\sim$ & Normal$(0,\ 0.25)$ & 0\\
    $\mu_{2} $ & $\sim$ & Normal$(0,\ 0.25)$ & 0\\
    $\sigma^2_{0} $ & $\sim$ & Gamma$(10,\ 1)$ & 10 \\
    $\sigma^2_{1} $ & $\sim$ & Gamma$(3,\ 1)$ & 3 \\
    $\sigma^2_{2} $ & $\sim$ & Gamma$(3,\ 1)$ & 3 \\
    $p_{2,\cdot}$ & $\sim$ & Dirichlet$(5,\ 85,\ 5,\ 5)$ & 0.05, 0.85, 0.05, 0.05 \\
    $p_{3,\cdot}$ & $\sim$ & Beta$(2,\ 8)$ & 0.20\\
    $p_{4,\cdot}$ & $\sim$ & Beta$(2,\ 8)$ & 0.20\\
    \bottomrule
  \end{tabular}
  \caption{Informative prior distributions for Chopin's Mazurka Op. 63 No.3}
  \label{tab:priors}
\end{table}

The results for one performance by Sviatislav Richter are shown in Figure \ref{fig:richter1976} with the results of the other performances found in the appendix. The black line traces the observed dynamics for each note across time in accordance with their note onset.  The colored dots are used to differentiate the 4 possible discrete states and show the interpreted musical dynamics for each note.  The background is shaded based on musical directions given by the composer in the score.  Notice with this performance, the model often seems to start a new smooth progression when the composer gives direction about the dynamics.  There are occasions where this is not always the case.  Notice that Richter appears to smoothly adjust the dynamics from the first "forte" section into the first "piano" section.  Then he proceeds with a couple of abrupt changes to the dynamics although keeping it relavitely "soft".  These unscripted parts of the performance are what we hope to discover as they can provide an explanation for why one person might prefer one performance over another.

```{r,richter1976,echo=FALSE,fig.margin=TRUE,fig.cap="\\label{fig:richter1976}Interpreted Dynamics from Richter's 1976 performance of Chopin's Mazurka Op. 63 No. 3.  The black line traces the observed dynamics while the colored dots are the smoothed interpreted dynamics."}
library(dpf)
set.seed(617)
yt <- matrix(dynamics[,'Richter_1976'], 1)
lt = diff(c(dynamics$note_onset, 61))
theta <- c(10.783294628,  4.260923847,
           21.840403351, -0.135257595, -0.022207528,
           12.245382937,  0.222490695,  0.003387143,
           0.050233166,   0.016500239,  0.043559623,  0.106812768,  0.048355188)
pmats = musicModeldynamics(lt, theta[1], theta[2], theta[3:5], theta[6:8], theta[9:13], c(18,0,0), c(10,2,2)) 
beam = with(pmats, beamSearch(a0, P0, c(1,0,0,0), dt, ct, Tt, Zt,
                              HHt, GGt, yt, transMat, 500, samplemethod = 1))
states = beam$paths[which.max(beam$weights),]

plotStates2(theta,states,yt,dynamics$note_onset,model="dynamics",c(18,0,0), c(10,2,2),title='')
```


The final important piece of this model that can be used to characterize a performance is the parameter values, $\Theta$.  The parameters for this particular performance can be found in \autoref{tab:parameterestimatesrichter}, and all parameter estimates for the 46 performances can be found in the appendix.  Certain parameters can be very informative in differentiating performances.  For example, large values of $\mu_c$ will tell us that the performer often uses very loud or very soft points of emphasis, whereas smaller values indicate the performer is more subtle.  If we want to get some idea as to the overall loudness of a piece, we may take a look at $\mu_0$.  The parameter values, along with the interpreted dynamics and discrete state, provide considerable information regarding differentiating between performances.

\begin{table}[t]
  \centering
  \begin{tabular}{@{}rcccccccc@{}}
    \toprule
    $\mu_c$ & $\sigma^2_{\epsilon}$ & $\mu_{0}$ & $\mu_{1}$ & $\mu_{2}$ & $\sigma^2_{0} $ & $\sigma^2_{1} $ & $\sigma^2_{2}$ \\
    \midrule
    $10.7833$ & $4.2609$ & $21.8404$ & $-0.1353$ & $-0.0222$ & $12.2454$ & $0.2225$ & $0.0034$ \\
    \midrule
    $p_{21}$ & $p_{23}$ & $p_{24}$ & $p_{31}$ & $p_{41}$ \\
    \midrule
    $0.0502$ & $0.0165$ & $0.0436$ & $0.1068$ & $0.04836$ \\
    \bottomrule
  \end{tabular}
  \caption{Parameter Estimates for Richter's 1976 Performance of Chopin's Mazurka Op. 63 No.3}
  \label{tab:parameterestimatesrichter}
\end{table}

\section{Conclusion}
\label{sec:conclusion}

\nocite{*}


